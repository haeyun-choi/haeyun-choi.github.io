---
---

@string{CVPR = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)}}
@string{ICCV = {{IEEE/CVF} Conference on International Conference on Computer Vision (<b>ICCV</b>)}}
@string{ECCV = {European Conference on Computer Vision (<b>ECCV</b>)}}
@string{IJCV = {International Journal of Computer Vision (<b>IJCV</b>),}}
@string{ICRA = {IEEE International Conference on Robotics and Automation (<b>ICRA</b>),}}
@string{ICML = {International Conference on Machine Learning (<b>ICML</b>),}}
@string{WACV = {IEEE/CVF Winter Conference on Applications of Computer Vision(<b>IJCV</b>),}}
@string{BMVC = {British Machine Vision Conference (<b>BMVC</b>),}}
@string{NeurIPS = {Conference on Neural Information Processing Systems (<b>NeurIPS</b>),}}
@string{NeurIPS-W = {Conference on Neural Information Processing Systems Workshop (<b>NeurIPS-W</b>),}}
@string{NeurIPS-D = {Conference on Neural Information Processing Systems Dataset and Benchmark (<b>NeurIPS-D</b>),}}
@string{arXiv = {arXiv preprint}}
@string{OpenReview = {OpenReview preprint}}
@string{IPIU = {Workshop on Image Processing and Image Understanding (<b>IPIU</b>)}}

@inproceedings{choi2025exploiting,
  title={Exploiting Deblurring Networks for Radiance Fields},
  author={Haeyun Choi and Heemin Yang and Janghyuk Han and Sunghyun Cho},
  abstract={In this paper, we propose DeepDeblurRF, a novel radiance field deblurring approach that can synthesize high-quality novel views from blurred training views with significantly reduced training time. DeepDeblurRF leverages deep neural network (DNN)-based deblurring modules to enjoy their deblurring performance and computational efficiency. To effectively combine DNN-based deblurring and radiance f ield construction, we propose a novel radiance field (RF)guided deblurring and an iterative framework that performs RF-guided deblurring and radiance field construction in an alternating manner. Moreover, DeepDeblurRF is compatible with various scene representations, such as voxel grids and 3D Gaussians, expanding its applicability. We also present BlurRF-Synth, the first large-scale synthetic dataset for training radiance field deblurring frameworks. We conduct extensive experiments on both camera motion blur and defocus blur, demonstrating that DeepDeblurRF achieves state-of-the-art novel-view synthesis quality with significantly reduced training time.},
  booktitle=CVPR,
  year={2025},
  website={https://haeyun-choi.github.io/DDRF_site},
  code={https://haeyun-choi.github.io/},
  abbr={CVPR},
  arxiv={2502.14454},
  code={https://github.com/haeyun-choi/DeepDeblurRF},
  selected={true}, 
  preview={deepdeblurrf.jpg},
  note={<span style="color: red;">Outstanding Paper Award at IPIU 2025</span>}
}

@inproceedings{choi2024optical,
  title={Optical Flow-based Shakiness Detection in Dash-cams for Hit-and-Run Accident Analysis},
  author={Haeyun Choi and Sunghyun Cho},
  abstract={In this paper, we present a straightforward approach for detecting video shakiness in dash-cams and demonstrate its potential application in analyzing hit-and-run accidents. We use optical flow to calculate directional components of motion vectors for each key point, uniformly distributed across every frame. Subsequently, we leverage the trend of the standard deviation of these motion vectors' directions to detect occurrences of shake in dash-cam footage. Since there is no publicly available dataset related to hit-and-run accidents, we collected a total of 63 dash-cam videos to evaluate the performance of our algorithm. The experimental results show that the proposed method achieves high detection performance across various types and levels of hit-and-run accidents even under low-light or nighttime conditions.},
  booktitle=IPIU,
  year={2024},
  abbr={IPIU},
  pdf={https://drive.google.com/file/d/1Vq19q-U6sxwd0jW8TQ_fpUtJkmuP0KgO/view},
  selected={true},
  preview={optical.png}
}

@inproceedings{choi2023autocycle,
  title={AutoCycle-VC: Towards Bottleneck-Independent Zero-Shot Cross-Lingual Voice Conversion},
  author={Haeyun Choi and Jio Gim and Yuho Lee and Youngin Kim and Youngjoo Suh},
  abstract={This paper proposes a simple and robust zero-shot voice conversion system with a cycle structure and mel-spectrogram pre-processing. Previous works suffer from information loss and poor synthesis quality due to their reliance on a carefully designed bottleneck structure. Moreover, models relying solely on self-reconstruction loss struggled with reproducing different speakers' voices. To address these issues, we suggested a cycle-consistency loss that considers conversion back and forth between target and source speakers. Additionally, stacked random-shuffled mel-spectrograms and a label smoothing method are utilized during speaker encoder training to extract a time-independent global speaker representation from speech, which is the key to a zero-shot conversion. Our model outperforms existing state-of-the-art results in both subjective and objective evaluations. Furthermore, it facilitates cross-lingual voice conversions and enhances the quality of synthesized speech.},
  booktitle=arXiv,
  year={2023},
  abbr={arXiv},
  arxiv={2310.06546},
  selected={true},
  preview={autocyclevc.png}
}


