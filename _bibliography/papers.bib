---
---

@string{CVPR = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)}}
@string{ICCV = {{IEEE/CVF} Conference on International Conference on Computer Vision (<b>ICCV</b>)}}
@string{ECCV = {European Conference on Computer Vision (<b>ECCV</b>)}}
@string{IJCV = {International Journal of Computer Vision (<b>IJCV</b>),}}
@string{ICRA = {IEEE International Conference on Robotics and Automation (<b>ICRA</b>),}}
@string{ICML = {International Conference on Machine Learning (<b>ICML</b>),}}
@string{WACV = {IEEE/CVF Winter Conference on Applications of Computer Vision(<b>IJCV</b>),}}
@string{BMVC = {British Machine Vision Conference (<b>BMVC</b>),}}
@string{NeurIPS = {Conference on Neural Information Processing Systems (<b>NeurIPS</b>),}}
@string{NeurIPS-W = {Conference on Neural Information Processing Systems Workshop (<b>NeurIPS-W</b>),}}
@string{NeurIPS-D = {Conference on Neural Information Processing Systems Dataset and Benchmark (<b>NeurIPS-D</b>),}}
@string{arXiv = {arXiv preprint}}
@string{OpenReview = {OpenReview preprint}}

@inproceedings{choi2025exploiting,
  title={Exploiting Deblurring Networks for Radiance Fields},
  author={Haeyun Choi and Heemin Yang and Janghyuk Han and Sunghyun Cho},
  abstract={In this paper, we propose DeepDeblurRF, a novel radiance f ield deblurring approach that can synthesize high-quality novel views from blurred training views with significantly reduced training time. DeepDeblurRF leverages deep neural network (DNN)-based deblurring modules to enjoy their deblurring performance and computational efficiency. To effectively combine DNN-based deblurring and radiance f ield construction, we propose a novel radiance field (RF)guided deblurring and an iterative framework that performs RF-guided deblurring and radiance field construction in an alternating manner. Moreover, DeepDeblurRF is compatible with various scene representations, such as voxel grids and 3D Gaussians, expanding its applicability. We also present BlurRF-Synth, the first large-scale synthetic dataset for training radiance field deblurring frameworks. We conduct extensive experiments on both camera motion blur and defocus blur, demonstrating that DeepDeblurRF achieves state-of-the-art novel-view synthesis quality with significantly reduced training time.},
  booktitle=CVPR,
  year={2025},
  abbr={CVPR},
  arxiv={2502.14454},
  selected={true},
  img_path={assets/img/deepdeblurrf.png}
}

@inproceedings{choi2023autocycle,
  title={AutoCycle-VC: Towards Bottleneck-Independent Zero-Shot Cross-Lingual Voice Conversion},
  author={Haeyun Choi and Jio Gim and Yuho Lee and Youngin Kim and Youngjoo Suh},
  abstract={This paper proposes a simple and robust zeroshot voice conversion system with a cycle arXiv:2310.06546v1  [cs.SD]  10 Oct 2023 structure and mel-spectrogram pre-processing. Previous works suffer from information loss and poor synthesis quality due to their reliance on a carefully designed bottleneck structure. Moreover, models relying solely on selfreconstruction loss struggled with reproducing different speakersâ€™ voices. To address these issues, we suggested a cycle-consistency loss that considers conversion back and forth between target and source speakers. Additionally, stacked random-shuffled mel-spectrograms and a label smoothing method are utilized during speaker encoder training to extract a timeindependent global speaker representation from speech, which is the key to a zero-shot conversion. Our model outperforms existing stateof-the-art results in both subjective and objective evaluations. Furthermore, it facilitates cross-lingual voice conversions and enhances the quality of synthesized speech.},
  booktitle=arXiv,
  year={2023},
  abbr={arXiv},
  arxiv={2310.06546},
  selected={true},
  img_path={assets/img/autocyclevc.png}
}


